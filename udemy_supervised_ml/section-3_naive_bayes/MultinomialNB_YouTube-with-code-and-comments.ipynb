{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebd4364",
   "metadata": {},
   "source": [
    "# Multinomial Na√Øve Bayes Classifier - the YouTube Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053831be",
   "metadata": {},
   "source": [
    "### Introducing the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b3cb7",
   "metadata": {},
   "source": [
    "The database for this example is taken from https://archive.ics.uci.edu/ml/machine-learning-databases/00380/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85d311",
   "metadata": {},
   "source": [
    "We usually modify the databases slightly such that they fit the purpose of the course. Therefore, we suggest you use the database provided in the resources in order to obtain the same results as the ones in the lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e299f9e4",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca2a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A module for handling data\n",
    "import pandas as pd\n",
    "# A module that helps finding all pathnames that match a certain pattern\n",
    "import glob\n",
    "\n",
    "# A class that will be used to count the number of times a word has occurred in a text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# A method used to split the dataset into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The multinomial type of the Naive Bayes classfier\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "\n",
    "# Importing different metrics that would allow us to evaluate our model\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Python's plotting module. \n",
    "# We improve the graphics by overriding the default matplotlib styles with those of seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# The Python package for scientific computing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec59c4c3",
   "metadata": {},
   "source": [
    "### Reading the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 'glob()' method, create a variable of type 'list' called 'files'. \n",
    "# It stores the paths of all files in the folder 'youtube-dataset' whose extension is .csv.\n",
    "files = glob.glob('youtube-dataset\\\\*.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An empty array which will be used to store all 5 dataframes corresponding to the 5 .csv files. \n",
    "all_df = []\n",
    "\n",
    "# Run a for-loop where the iterator 'i' goes through each filename in the 'files' array.\n",
    "# During each iteration, create a pandas DataFrame by reading the current .csv file. \n",
    "# Drop the unneccesary columns (along axis 1) and append the dataframe to the 'all_df' list.\n",
    "for i in files:\n",
    "    all_df.append(pd.read_csv(i).drop(['COMMENT_ID', 'AUTHOR', 'DATE'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf721dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first item in the list\n",
    "all_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f011d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the list\n",
    "len(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3989fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that combines all pandas dataframes from the 'all_df' list\n",
    "data = pd.concat(all_df, axis=0, ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a7bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values. In this case, there aren't any.\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2096bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of representatives from each class. \n",
    "# In this case, the data is reasonably balanced.\n",
    "data['CLASS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbfe36a",
   "metadata": {},
   "source": [
    "### Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list storing a single string\n",
    "message_sample = ['This is a dog']\n",
    "\n",
    "# Define an instance of the CountVectorizer class\n",
    "vectorizer_sample = CountVectorizer()\n",
    "\n",
    "# Learn a vocabulary dictionary of all tokens in 'message_sample'. A token is a word consisting of at least 2 letters.\n",
    "vectorizer_sample.fit(message_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35db8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the document into a document-term matrix. \n",
    "# Each 1 in the matrix represents the presence of a certain word in the fitted string(s).\n",
    "vectorizer_sample.transform(message_sample).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the words that represent each column in the matrix above.\n",
    "# For example, the first 1 in the matrix above corresponds to the presence of the word 'dog' in the string.\n",
    "# The 2nd 1 means that the string also includes the word 'is'.\n",
    "# The 3rd one means that the word 'this' is also present in the string\n",
    "# Notice how the article 'a' is not included in the list of tokens, as it consists of only one letter.\n",
    "vectorizer_sample.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the 'transform()' method to a new string and check which of the tokenized words are present.\n",
    "# We can see a 0 in the first column.\n",
    "# This indicate that the word 'dog' is absent from the transformed string.\n",
    "vectorizer_sample.transform(['This is a cat']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04cfde1",
   "metadata": {},
   "source": [
    "### Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc38d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list storing two strings\n",
    "message_sample2 = ['This is a dog and that is a dog', 'This is a cat']\n",
    "\n",
    "# Define another instance of the CountVectorizer class\n",
    "vectorizer_sample2 = CountVectorizer()\n",
    "\n",
    "# Apply the 'fit()' and the 'transform()' methods simultaneously\n",
    "vectorizer_sample2.fit_transform(message_sample2).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the words that represent each column in the matrix above.\n",
    "# The elements in the first column are 0 and 1.\n",
    "# This means that the word 'cat' is not present in the first string but is found in the second string.\n",
    "# The elements in the second column are 1 and 0.\n",
    "# Therefore, the word 'dog' is present in the first string, but not in the second.\n",
    "# We appply the same arguments to the elements of the third and the fourth column in the matrix.\n",
    "vectorizer_sample2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c86cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the 'transform()' method to a new string and check which of the tokenized words are present.\n",
    "# All columns are 0. Therefore, none of the words above are present in this string.\n",
    "vectorizer_sample2.transform(['Those are birds.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167043b",
   "metadata": {},
   "source": [
    "### Defining the inputs and the target. Creating the train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the inputs and the target\n",
    "inputs = data['CONTENT']\n",
    "target = data['CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfcd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training and a testing dataset.\n",
    "# Choose the test size such that 20% of the data goes to testing.\n",
    "# Since 'train_test_split()' distributes the points randomly, we set a seed equal to 365\n",
    "# so that the final results are identical each time we run the split.\n",
    "# The 'stratify' argument allows for splitting the data in such a way that\n",
    "# the training and the testing datasets contain an equal portion of samples\n",
    "# from both classes.\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, target, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=365, \n",
    "                                                    stratify = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f87086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the counts on the ham and the spam messages in the training dataset.\n",
    "# You can choose to normalize the counts or display their actual value.\n",
    "# y_train.value_counts(normalize = True)\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the counts on the ham and the spam messages in the test dataset.\n",
    "# You can choose to normalize the counts or display their actual value\n",
    "# y_test.value_counts(normalize = True)\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8a85e",
   "metadata": {},
   "source": [
    "### Tokenizing the YouTube comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the vectorizer class.\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the fit_transform() method on the training data and the transform() method on the test data.\n",
    "# Note that we split the data *before* applying the 'fit_tranform()' method!\n",
    "# The 'fit_transform()' method is to be applied *only* on the training data!\n",
    "x_train_transf = vectorizer.fit_transform(x_train)\n",
    "x_test_transf = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how the 'x_train_transf' matrix looks like.\n",
    "# It is a sparse matrix i.e., contains mainly zeros.\n",
    "x_train_transf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1564 refers to all comments in the training dataset.\n",
    "# 3925 refers to all words that have been tokenized.\n",
    "x_train_transf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01926f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 391 refers to all comments in the test dataset.\n",
    "# 3925 refers to all words that have been tokenized during the fitting process.\n",
    "x_test_transf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11cdb00",
   "metadata": {},
   "source": [
    "### Performing the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce116cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "# clf = MultinomialNB(class_prior = np.array([0.6, 0.4]))\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(x_train_transf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6bd4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the class priors.\n",
    "np.exp(clf.class_log_prior_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the parameters of the classifier\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d523f",
   "metadata": {},
   "source": [
    "### Performing the evaluation on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a3d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target of the observations in the test set\n",
    "y_test_pred = clf.predict(x_test_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344760ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the seaborn style, so that we remove the white lines passing throguh the numbers \n",
    "sns.reset_orig()\n",
    "\n",
    "# Create the confusion matrix by providing the true and the predicted values as arguments.\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_test_pred,\n",
    "    labels = clf.classes_,\n",
    "    cmap = 'magma'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classification report consists of the precision, recall and f1-score of each class as well as\n",
    "# the overall accuracy of the model.\n",
    "print(classification_report(y_test, y_test_pred, target_names = ['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ec641",
   "metadata": {},
   "source": [
    "### Creating probability-distribution figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01357a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each ovservation, store its probability of being a spam. Round it to 3 digits after the decimal points.\n",
    "# These probabilities will later serve as the x-coordinates of a scatter plot\n",
    "spam_proba = clf.predict_proba(x_test_transf).round(3)[:,1];\n",
    "\n",
    "# Create a new data frame which will store the true classes, the predicted classes and the predicted probability\n",
    "# for an observation to belong to the spam class\n",
    "df_scatter = pd.DataFrame()\n",
    "\n",
    "# Create the columns of the data frame\n",
    "df_scatter['True class'] = y_test\n",
    "df_scatter['Predicted class'] = y_test_pred\n",
    "df_scatter['Predicted probability (spam)'] = spam_proba\n",
    "\n",
    "# Reset the index of the data frame, so that the indexing is sequential\n",
    "df_scatter = df_scatter.reset_index(drop = True)\n",
    "\n",
    "# Define two palettes for the different plots\n",
    "palette_0 = sns.color_palette(['#000000'])\n",
    "palette_1 = sns.color_palette(['#FF0000'])\n",
    "\n",
    "# Create two new data frames. The first one stores the samples whose true class is 0.\n",
    "# The second one stores the samples whose true class is 1.\n",
    "df_scatter_0 = df_scatter[df_scatter['True class'] == 0].reset_index(drop = True)\n",
    "df_scatter_1 = df_scatter[df_scatter['True class'] == 1].reset_index(drop = True)\n",
    "\n",
    "# Set the seaborn style\n",
    "sns.set()\n",
    "\n",
    "# Create two subfigures, so that they are arranged in 2 rows and 1 column.\n",
    "# Set the size of the figures\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12,5))\n",
    "fig.tight_layout(pad = 3)\n",
    "\n",
    "# Create a scatter plot.\n",
    "# On the x-axis, we have the predicted probability of a comment being a spam.\n",
    "# The y-axis takes only one value - zero. We create an array full of zeros having a length equal to \n",
    "# the number of samples whose true class is 0.\n",
    "# The 'data' parameter specifies the dataset we are drawing the columns from.\n",
    "# The 'hue' parameter specifies the feature based on which the points are going to be colored. In this case,\n",
    "# this feature has only a single class - the 0 class.\n",
    "# The 's' parameter specifies the size of the points.\n",
    "# The 'markers' parameter determines the shape of the points.\n",
    "# The 'palette' parameter specifies the colors to be used in the plot.\n",
    "# The 'style' parameter connects markers to classes.\n",
    "# We have decided to set the 'legend' parameter equal to False as we have only one class in the plot. \n",
    "# We decide to remove the ticks from the y-axis as they can be chosen arbitrarily.\n",
    "sns.scatterplot(x = 'Predicted probability (spam)', \n",
    "                y = np.zeros(df_scatter_0.shape[0]), \n",
    "                data = df_scatter_0,\n",
    "                hue = 'True class', \n",
    "                s = 50,\n",
    "                markers = ['o'],\n",
    "                palette = palette_0,\n",
    "                style = 'True class',\n",
    "                legend = False, \n",
    "                ax = ax1).set(yticklabels=[])\n",
    "\n",
    "# Set a title above the first plot.\n",
    "ax1.set_title('Probability distribution of comments belonging to the true \\'ham\\' class')\n",
    "\n",
    "# Create a red vertical dashed line passing through the point x = 0.5\n",
    "# The first parameter specifies the x-coordinate.\n",
    "# The second and third parameters specify the min and the max y-coordinates.\n",
    "# We choose a dashed linestyle.\n",
    "# We color the line in red.\n",
    "ax1.vlines(0.5, -1, 1, linestyles = 'dashed', colors = 'red');\n",
    "\n",
    "# The comments above apply to this plot. The code is adapted to the spam class\n",
    "sns.scatterplot(x = 'Predicted probability (spam)', \n",
    "                y = np.zeros(df_scatter_1.shape[0]), \n",
    "                hue = 'True class', \n",
    "                data = df_scatter_1,\n",
    "                s = 50,\n",
    "                palette = palette_1,\n",
    "                markers = ['X'],\n",
    "                style = 'True class',\n",
    "                legend = False, \n",
    "                ax = ax2).set(yticklabels=[])\n",
    "\n",
    "# Set a title above the second plot.\n",
    "ax2.set_title('Probability distribution of comments belonging to the true \\'spam\\' class')\n",
    "\n",
    "# Create a red vertical dahsed line for the second plot\n",
    "ax2.vlines(0.5, -1, 1, linestyles = 'dashed', colors = 'red');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6fb120",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d261220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the messages whose class you want to predict and put them in a list.\n",
    "# Using the vectorizer, transform the messages and store the result in a variable called 'predict_data'.\n",
    "predict_data = vectorizer.transform(['This song is amazing!',\n",
    "                                     'You can win 1m dollars right now, just click here!!!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861be544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the 'predict()' method and use 'predict_data' as an argument.\n",
    "# The classifier correctly predicts the two messages.\n",
    "clf.predict(predict_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
