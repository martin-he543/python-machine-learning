{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.pieriandata.com\"><img src=\"../Pierian_Data_Logo.PNG\"></a>\n",
    "<strong><center>Copyright by Pierian Data Inc.</center></strong> \n",
    "<strong><center>Created by Jose Marcial Portilla.</center></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPLOYMENT\n",
    "\n",
    "**Welcome to deployment section! In this section of the course, we will go through the entire deployment process, starting as if you had to create a servicable model from scratch, then deploy it for others to use, either through API or a web form.**\n",
    "\n",
    "# Data\n",
    "\n",
    "For this example we use the very common data set: [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set), which is about flowers. \n",
    "\n",
    "From Wikipedia:\n",
    "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.[1] It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species.[2] Two of the three species were collected in the Gasp√© Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\".[3]\n",
    "\n",
    "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv(\"../DATA/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n",
       "       'species'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.drop('species',axis=1)\n",
    "y = iris['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lots of ways to one hot encode\n",
    "# https://stackoverflow.com/questions/47573293/unable-to-transform-string-column-to-categorical-matrix-using-keras-and-sklearn\n",
    "# https://stackoverflow.com/questions/35107559/one-hot-encoding-of-string-categorical-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "\n",
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=4,activation='relu',input_shape=[4,]))\n",
    "\n",
    "# Last layer for multi-class classification of 3 species\n",
    "model.add(Dense(units=3,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/300\n",
      "120/120 [==============================] - 1s 7ms/sample - loss: 1.1243 - accuracy: 0.3250 - val_loss: 1.1015 - val_accuracy: 0.4000\n",
      "Epoch 2/300\n",
      "120/120 [==============================] - 0s 200us/sample - loss: 1.1181 - accuracy: 0.3167 - val_loss: 1.0980 - val_accuracy: 0.3667\n",
      "Epoch 3/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 1.1127 - accuracy: 0.3000 - val_loss: 1.0945 - val_accuracy: 0.3667\n",
      "Epoch 4/300\n",
      "120/120 [==============================] - 0s 192us/sample - loss: 1.1071 - accuracy: 0.3000 - val_loss: 1.0909 - val_accuracy: 0.4000\n",
      "Epoch 5/300\n",
      "120/120 [==============================] - 0s 200us/sample - loss: 1.1018 - accuracy: 0.3000 - val_loss: 1.0875 - val_accuracy: 0.4000\n",
      "Epoch 6/300\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 1.0963 - accuracy: 0.2833 - val_loss: 1.0840 - val_accuracy: 0.4333\n",
      "Epoch 7/300\n",
      "120/120 [==============================] - 0s 200us/sample - loss: 1.0917 - accuracy: 0.2750 - val_loss: 1.0811 - val_accuracy: 0.4000\n",
      "Epoch 8/300\n",
      "120/120 [==============================] - 0s 206us/sample - loss: 1.0871 - accuracy: 0.2750 - val_loss: 1.0786 - val_accuracy: 0.4000\n",
      "Epoch 9/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.0830 - accuracy: 0.2750 - val_loss: 1.0765 - val_accuracy: 0.3667\n",
      "Epoch 10/300\n",
      "120/120 [==============================] - 0s 217us/sample - loss: 1.0794 - accuracy: 0.2833 - val_loss: 1.0745 - val_accuracy: 0.3667\n",
      "Epoch 11/300\n",
      "120/120 [==============================] - 0s 217us/sample - loss: 1.0764 - accuracy: 0.2833 - val_loss: 1.0728 - val_accuracy: 0.3667\n",
      "Epoch 12/300\n",
      "120/120 [==============================] - 0s 163us/sample - loss: 1.0731 - accuracy: 0.2667 - val_loss: 1.0713 - val_accuracy: 0.3667\n",
      "Epoch 13/300\n",
      "120/120 [==============================] - 0s 225us/sample - loss: 1.0698 - accuracy: 0.2667 - val_loss: 1.0696 - val_accuracy: 0.3667\n",
      "Epoch 14/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 1.0672 - accuracy: 0.3167 - val_loss: 1.0680 - val_accuracy: 0.4667\n",
      "Epoch 15/300\n",
      "120/120 [==============================] - 0s 217us/sample - loss: 1.0646 - accuracy: 0.3500 - val_loss: 1.0664 - val_accuracy: 0.4667\n",
      "Epoch 16/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.0622 - accuracy: 0.3333 - val_loss: 1.0648 - val_accuracy: 0.4667\n",
      "Epoch 17/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.0600 - accuracy: 0.3500 - val_loss: 1.0631 - val_accuracy: 0.4333\n",
      "Epoch 18/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 1.0580 - accuracy: 0.3583 - val_loss: 1.0615 - val_accuracy: 0.4333\n",
      "Epoch 19/300\n",
      "120/120 [==============================] - 0s 192us/sample - loss: 1.0560 - accuracy: 0.3667 - val_loss: 1.0598 - val_accuracy: 0.4333\n",
      "Epoch 20/300\n",
      "120/120 [==============================] - 0s 200us/sample - loss: 1.0541 - accuracy: 0.3667 - val_loss: 1.0584 - val_accuracy: 0.3667\n",
      "Epoch 21/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.0521 - accuracy: 0.3500 - val_loss: 1.0568 - val_accuracy: 0.3667\n",
      "Epoch 22/300\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 1.0501 - accuracy: 0.3583 - val_loss: 1.0551 - val_accuracy: 0.4000\n",
      "Epoch 23/300\n",
      "120/120 [==============================] - 0s 225us/sample - loss: 1.0481 - accuracy: 0.3750 - val_loss: 1.0534 - val_accuracy: 0.4000\n",
      "Epoch 24/300\n",
      "120/120 [==============================] - 0s 233us/sample - loss: 1.0460 - accuracy: 0.3833 - val_loss: 1.0515 - val_accuracy: 0.4000\n",
      "Epoch 25/300\n",
      "120/120 [==============================] - 0s 225us/sample - loss: 1.0439 - accuracy: 0.4083 - val_loss: 1.0497 - val_accuracy: 0.4333\n",
      "Epoch 26/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.0419 - accuracy: 0.4250 - val_loss: 1.0480 - val_accuracy: 0.4333\n",
      "Epoch 27/300\n",
      "120/120 [==============================] - 0s 200us/sample - loss: 1.0396 - accuracy: 0.4417 - val_loss: 1.0460 - val_accuracy: 0.4333\n",
      "Epoch 28/300\n",
      "120/120 [==============================] - 0s 200us/sample - loss: 1.0374 - accuracy: 0.4750 - val_loss: 1.0442 - val_accuracy: 0.4333\n",
      "Epoch 29/300\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 1.0351 - accuracy: 0.4667 - val_loss: 1.0423 - val_accuracy: 0.4333\n",
      "Epoch 30/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.0328 - accuracy: 0.4583 - val_loss: 1.0403 - val_accuracy: 0.4333\n",
      "Epoch 31/300\n",
      "120/120 [==============================] - 0s 200us/sample - loss: 1.0306 - accuracy: 0.4500 - val_loss: 1.0385 - val_accuracy: 0.4333\n",
      "Epoch 32/300\n",
      "120/120 [==============================] - 0s 200us/sample - loss: 1.0281 - accuracy: 0.4583 - val_loss: 1.0365 - val_accuracy: 0.4333\n",
      "Epoch 33/300\n",
      "120/120 [==============================] - 0s 200us/sample - loss: 1.0258 - accuracy: 0.4500 - val_loss: 1.0346 - val_accuracy: 0.4000\n",
      "Epoch 34/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.0233 - accuracy: 0.4583 - val_loss: 1.0323 - val_accuracy: 0.4000\n",
      "Epoch 35/300\n",
      "120/120 [==============================] - 0s 200us/sample - loss: 1.0210 - accuracy: 0.4750 - val_loss: 1.0303 - val_accuracy: 0.4333\n",
      "Epoch 36/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.0184 - accuracy: 0.4833 - val_loss: 1.0284 - val_accuracy: 0.4667\n",
      "Epoch 37/300\n",
      "120/120 [==============================] - 0s 208us/sample - loss: 1.0159 - accuracy: 0.4917 - val_loss: 1.0261 - val_accuracy: 0.4667\n",
      "Epoch 38/300\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 1.0134 - accuracy: 0.5000 - val_loss: 1.0240 - val_accuracy: 0.4667\n",
      "Epoch 39/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 1.0108 - accuracy: 0.5000 - val_loss: 1.0220 - val_accuracy: 0.4667\n",
      "Epoch 40/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 1.0082 - accuracy: 0.5083 - val_loss: 1.0198 - val_accuracy: 0.4667\n",
      "Epoch 41/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 1.0056 - accuracy: 0.5250 - val_loss: 1.0176 - val_accuracy: 0.5000\n",
      "Epoch 42/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 1.0029 - accuracy: 0.5167 - val_loss: 1.0154 - val_accuracy: 0.5000\n",
      "Epoch 43/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 1.0002 - accuracy: 0.5250 - val_loss: 1.0132 - val_accuracy: 0.5000\n",
      "Epoch 44/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.9976 - accuracy: 0.5250 - val_loss: 1.0112 - val_accuracy: 0.5000\n",
      "Epoch 45/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.9948 - accuracy: 0.5417 - val_loss: 1.0090 - val_accuracy: 0.5000\n",
      "Epoch 46/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.9921 - accuracy: 0.5250 - val_loss: 1.0068 - val_accuracy: 0.4333\n",
      "Epoch 47/300\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 0.9894 - accuracy: 0.5167 - val_loss: 1.0045 - val_accuracy: 0.4000\n",
      "Epoch 48/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.9866 - accuracy: 0.5167 - val_loss: 1.0021 - val_accuracy: 0.4000\n",
      "Epoch 49/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.9837 - accuracy: 0.5167 - val_loss: 0.9998 - val_accuracy: 0.4333\n",
      "Epoch 50/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.9810 - accuracy: 0.5000 - val_loss: 0.9976 - val_accuracy: 0.4667\n",
      "Epoch 51/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.9781 - accuracy: 0.5083 - val_loss: 0.9951 - val_accuracy: 0.4333\n",
      "Epoch 52/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.9752 - accuracy: 0.5167 - val_loss: 0.9926 - val_accuracy: 0.4333\n",
      "Epoch 53/300\n",
      "120/120 [==============================] - 0s 154us/sample - loss: 0.9723 - accuracy: 0.5250 - val_loss: 0.9901 - val_accuracy: 0.4333\n",
      "Epoch 54/300\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 0.9694 - accuracy: 0.5333 - val_loss: 0.9878 - val_accuracy: 0.4333\n",
      "Epoch 55/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.9664 - accuracy: 0.5250 - val_loss: 0.9854 - val_accuracy: 0.4333\n",
      "Epoch 56/300\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 0.9636 - accuracy: 0.5333 - val_loss: 0.9832 - val_accuracy: 0.4333\n",
      "Epoch 57/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.9606 - accuracy: 0.5500 - val_loss: 0.9808 - val_accuracy: 0.4333\n",
      "Epoch 58/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.9576 - accuracy: 0.5583 - val_loss: 0.9783 - val_accuracy: 0.4667\n",
      "Epoch 59/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.9546 - accuracy: 0.5667 - val_loss: 0.9758 - val_accuracy: 0.5000\n",
      "Epoch 60/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.9516 - accuracy: 0.5667 - val_loss: 0.9732 - val_accuracy: 0.5000\n",
      "Epoch 61/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.9487 - accuracy: 0.6000 - val_loss: 0.9709 - val_accuracy: 0.5000\n",
      "Epoch 62/300\n",
      "120/120 [==============================] - 0s 151us/sample - loss: 0.9455 - accuracy: 0.6083 - val_loss: 0.9684 - val_accuracy: 0.5000\n",
      "Epoch 63/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.9425 - accuracy: 0.6167 - val_loss: 0.9659 - val_accuracy: 0.5000\n",
      "Epoch 64/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.9395 - accuracy: 0.6000 - val_loss: 0.9635 - val_accuracy: 0.5000\n",
      "Epoch 65/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.9365 - accuracy: 0.6167 - val_loss: 0.9611 - val_accuracy: 0.5000\n",
      "Epoch 66/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.9334 - accuracy: 0.6333 - val_loss: 0.9587 - val_accuracy: 0.5000\n",
      "Epoch 67/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.9303 - accuracy: 0.6333 - val_loss: 0.9562 - val_accuracy: 0.5000\n",
      "Epoch 68/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.9272 - accuracy: 0.6417 - val_loss: 0.9537 - val_accuracy: 0.5000\n",
      "Epoch 69/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.9241 - accuracy: 0.6333 - val_loss: 0.9512 - val_accuracy: 0.5000\n",
      "Epoch 70/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.9210 - accuracy: 0.6417 - val_loss: 0.9488 - val_accuracy: 0.5000\n",
      "Epoch 71/300\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 0.9179 - accuracy: 0.6583 - val_loss: 0.9463 - val_accuracy: 0.5000\n",
      "Epoch 72/300\n",
      "120/120 [==============================] - 0s 175us/sample - loss: 0.9148 - accuracy: 0.6750 - val_loss: 0.9437 - val_accuracy: 0.5333\n",
      "Epoch 73/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.9117 - accuracy: 0.6750 - val_loss: 0.9413 - val_accuracy: 0.5667\n",
      "Epoch 74/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.9087 - accuracy: 0.6750 - val_loss: 0.9388 - val_accuracy: 0.5667\n",
      "Epoch 75/300\n",
      "120/120 [==============================] - 0s 165us/sample - loss: 0.9055 - accuracy: 0.6750 - val_loss: 0.9364 - val_accuracy: 0.5667\n",
      "Epoch 76/300\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 0.9024 - accuracy: 0.6750 - val_loss: 0.9340 - val_accuracy: 0.5667\n",
      "Epoch 77/300\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 0.8993 - accuracy: 0.6833 - val_loss: 0.9315 - val_accuracy: 0.5667\n",
      "Epoch 78/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.8962 - accuracy: 0.6833 - val_loss: 0.9290 - val_accuracy: 0.5667\n",
      "Epoch 79/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.8933 - accuracy: 0.6833 - val_loss: 0.9267 - val_accuracy: 0.5667\n",
      "Epoch 80/300\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 0.8901 - accuracy: 0.6833 - val_loss: 0.9242 - val_accuracy: 0.5667\n",
      "Epoch 81/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8870 - accuracy: 0.6833 - val_loss: 0.9218 - val_accuracy: 0.5667\n",
      "Epoch 82/300\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.8864 - accuracy: 0.75 - 0s 150us/sample - loss: 0.8840 - accuracy: 0.6833 - val_loss: 0.9195 - val_accuracy: 0.6000\n",
      "Epoch 83/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8810 - accuracy: 0.6833 - val_loss: 0.9171 - val_accuracy: 0.6000\n",
      "Epoch 84/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.8779 - accuracy: 0.6833 - val_loss: 0.9148 - val_accuracy: 0.6000\n",
      "Epoch 85/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8751 - accuracy: 0.6833 - val_loss: 0.9125 - val_accuracy: 0.6000\n",
      "Epoch 86/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8719 - accuracy: 0.6833 - val_loss: 0.9099 - val_accuracy: 0.6000\n",
      "Epoch 87/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8689 - accuracy: 0.6833 - val_loss: 0.9073 - val_accuracy: 0.6000\n",
      "Epoch 88/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.8659 - accuracy: 0.6833 - val_loss: 0.9049 - val_accuracy: 0.6000\n",
      "Epoch 89/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.8630 - accuracy: 0.6833 - val_loss: 0.9025 - val_accuracy: 0.6000\n",
      "Epoch 90/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8600 - accuracy: 0.6833 - val_loss: 0.9001 - val_accuracy: 0.6000\n",
      "Epoch 91/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.8570 - accuracy: 0.6833 - val_loss: 0.8976 - val_accuracy: 0.6000\n",
      "Epoch 92/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8541 - accuracy: 0.6833 - val_loss: 0.8952 - val_accuracy: 0.6000\n",
      "Epoch 93/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.8511 - accuracy: 0.6833 - val_loss: 0.8927 - val_accuracy: 0.6000\n",
      "Epoch 94/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.8483 - accuracy: 0.6833 - val_loss: 0.8903 - val_accuracy: 0.6000\n",
      "Epoch 95/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8454 - accuracy: 0.6833 - val_loss: 0.8878 - val_accuracy: 0.6000\n",
      "Epoch 96/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8426 - accuracy: 0.6833 - val_loss: 0.8855 - val_accuracy: 0.6000\n",
      "Epoch 97/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.8396 - accuracy: 0.6833 - val_loss: 0.8831 - val_accuracy: 0.6000\n",
      "Epoch 98/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8369 - accuracy: 0.6833 - val_loss: 0.8808 - val_accuracy: 0.6000\n",
      "Epoch 99/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.8340 - accuracy: 0.6833 - val_loss: 0.8782 - val_accuracy: 0.6000\n",
      "Epoch 100/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.8314 - accuracy: 0.6833 - val_loss: 0.8760 - val_accuracy: 0.6000\n",
      "Epoch 101/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.8284 - accuracy: 0.6833 - val_loss: 0.8736 - val_accuracy: 0.6000\n",
      "Epoch 102/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.8257 - accuracy: 0.6833 - val_loss: 0.8712 - val_accuracy: 0.6000\n",
      "Epoch 103/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8230 - accuracy: 0.6833 - val_loss: 0.8690 - val_accuracy: 0.6000\n",
      "Epoch 104/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8203 - accuracy: 0.6833 - val_loss: 0.8667 - val_accuracy: 0.6000\n",
      "Epoch 105/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.8176 - accuracy: 0.6833 - val_loss: 0.8644 - val_accuracy: 0.6000\n",
      "Epoch 106/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8148 - accuracy: 0.6833 - val_loss: 0.8621 - val_accuracy: 0.6000\n",
      "Epoch 107/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8123 - accuracy: 0.6833 - val_loss: 0.8599 - val_accuracy: 0.6000\n",
      "Epoch 108/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8096 - accuracy: 0.6833 - val_loss: 0.8575 - val_accuracy: 0.6000\n",
      "Epoch 109/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.8070 - accuracy: 0.6833 - val_loss: 0.8553 - val_accuracy: 0.6000\n",
      "Epoch 110/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.8044 - accuracy: 0.6833 - val_loss: 0.8531 - val_accuracy: 0.6000\n",
      "Epoch 111/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8018 - accuracy: 0.6833 - val_loss: 0.8509 - val_accuracy: 0.6000\n",
      "Epoch 112/300\n",
      "120/120 [==============================] - 0s 157us/sample - loss: 0.7992 - accuracy: 0.6833 - val_loss: 0.8486 - val_accuracy: 0.6000\n",
      "Epoch 113/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.7967 - accuracy: 0.6833 - val_loss: 0.8464 - val_accuracy: 0.6000\n",
      "Epoch 114/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7942 - accuracy: 0.6833 - val_loss: 0.8440 - val_accuracy: 0.6000\n",
      "Epoch 115/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.7919 - accuracy: 0.6833 - val_loss: 0.8421 - val_accuracy: 0.6000\n",
      "Epoch 116/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7892 - accuracy: 0.6833 - val_loss: 0.8399 - val_accuracy: 0.6000\n",
      "Epoch 117/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.7869 - accuracy: 0.6833 - val_loss: 0.8377 - val_accuracy: 0.6000\n",
      "Epoch 118/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.7843 - accuracy: 0.6833 - val_loss: 0.8354 - val_accuracy: 0.6000\n",
      "Epoch 119/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.7819 - accuracy: 0.6833 - val_loss: 0.8334 - val_accuracy: 0.6000\n",
      "Epoch 120/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7795 - accuracy: 0.6833 - val_loss: 0.8312 - val_accuracy: 0.6000\n",
      "Epoch 121/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.7771 - accuracy: 0.6833 - val_loss: 0.8291 - val_accuracy: 0.6000\n",
      "Epoch 122/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7747 - accuracy: 0.6833 - val_loss: 0.8271 - val_accuracy: 0.6000\n",
      "Epoch 123/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7725 - accuracy: 0.6833 - val_loss: 0.8251 - val_accuracy: 0.6000\n",
      "Epoch 124/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7702 - accuracy: 0.6833 - val_loss: 0.8232 - val_accuracy: 0.6000\n",
      "Epoch 125/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7678 - accuracy: 0.6833 - val_loss: 0.8211 - val_accuracy: 0.6000\n",
      "Epoch 126/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.7655 - accuracy: 0.6833 - val_loss: 0.8190 - val_accuracy: 0.6000\n",
      "Epoch 127/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.7633 - accuracy: 0.6833 - val_loss: 0.8170 - val_accuracy: 0.6000\n",
      "Epoch 128/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7610 - accuracy: 0.6833 - val_loss: 0.8149 - val_accuracy: 0.6000\n",
      "Epoch 129/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7588 - accuracy: 0.6833 - val_loss: 0.8128 - val_accuracy: 0.6000\n",
      "Epoch 130/300\n",
      "120/120 [==============================] - 0s 154us/sample - loss: 0.7566 - accuracy: 0.6833 - val_loss: 0.8105 - val_accuracy: 0.6000\n",
      "Epoch 131/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.7543 - accuracy: 0.6833 - val_loss: 0.8084 - val_accuracy: 0.6000\n",
      "Epoch 132/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.7521 - accuracy: 0.6833 - val_loss: 0.8065 - val_accuracy: 0.6000\n",
      "Epoch 133/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7499 - accuracy: 0.6833 - val_loss: 0.8045 - val_accuracy: 0.6000\n",
      "Epoch 134/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7478 - accuracy: 0.6833 - val_loss: 0.8022 - val_accuracy: 0.6000\n",
      "Epoch 135/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7456 - accuracy: 0.6833 - val_loss: 0.8003 - val_accuracy: 0.6000\n",
      "Epoch 136/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7435 - accuracy: 0.6833 - val_loss: 0.7982 - val_accuracy: 0.6000\n",
      "Epoch 137/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.7413 - accuracy: 0.6833 - val_loss: 0.7962 - val_accuracy: 0.6000\n",
      "Epoch 138/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7392 - accuracy: 0.6833 - val_loss: 0.7941 - val_accuracy: 0.6000\n",
      "Epoch 139/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7371 - accuracy: 0.6833 - val_loss: 0.7920 - val_accuracy: 0.6000\n",
      "Epoch 140/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7352 - accuracy: 0.6833 - val_loss: 0.7900 - val_accuracy: 0.6000\n",
      "Epoch 141/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.7330 - accuracy: 0.6833 - val_loss: 0.7880 - val_accuracy: 0.6000\n",
      "Epoch 142/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.7310 - accuracy: 0.6833 - val_loss: 0.7862 - val_accuracy: 0.6000\n",
      "Epoch 143/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.7290 - accuracy: 0.6833 - val_loss: 0.7843 - val_accuracy: 0.6000\n",
      "Epoch 144/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7270 - accuracy: 0.6833 - val_loss: 0.7825 - val_accuracy: 0.6000\n",
      "Epoch 145/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7250 - accuracy: 0.6833 - val_loss: 0.7806 - val_accuracy: 0.6000\n",
      "Epoch 146/300\n",
      "120/120 [==============================] - 0s 145us/sample - loss: 0.7231 - accuracy: 0.6833 - val_loss: 0.7787 - val_accuracy: 0.6000\n",
      "Epoch 147/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7212 - accuracy: 0.6833 - val_loss: 0.7772 - val_accuracy: 0.6000\n",
      "Epoch 148/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7192 - accuracy: 0.6833 - val_loss: 0.7754 - val_accuracy: 0.6000\n",
      "Epoch 149/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7172 - accuracy: 0.6833 - val_loss: 0.7736 - val_accuracy: 0.6000\n",
      "Epoch 150/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.7154 - accuracy: 0.6833 - val_loss: 0.7717 - val_accuracy: 0.6000\n",
      "Epoch 151/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7136 - accuracy: 0.6833 - val_loss: 0.7698 - val_accuracy: 0.6000\n",
      "Epoch 152/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.7116 - accuracy: 0.6833 - val_loss: 0.7682 - val_accuracy: 0.6000\n",
      "Epoch 153/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.7098 - accuracy: 0.6833 - val_loss: 0.7667 - val_accuracy: 0.6000\n",
      "Epoch 154/300\n",
      "120/120 [==============================] - 0s 154us/sample - loss: 0.7080 - accuracy: 0.6833 - val_loss: 0.7652 - val_accuracy: 0.6000\n",
      "Epoch 155/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7061 - accuracy: 0.6833 - val_loss: 0.7633 - val_accuracy: 0.6000\n",
      "Epoch 156/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7044 - accuracy: 0.6833 - val_loss: 0.7615 - val_accuracy: 0.6000\n",
      "Epoch 157/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7026 - accuracy: 0.6833 - val_loss: 0.7597 - val_accuracy: 0.6000\n",
      "Epoch 158/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.7008 - accuracy: 0.6833 - val_loss: 0.7580 - val_accuracy: 0.6000\n",
      "Epoch 159/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6991 - accuracy: 0.6833 - val_loss: 0.7564 - val_accuracy: 0.6000\n",
      "Epoch 160/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6973 - accuracy: 0.6833 - val_loss: 0.7549 - val_accuracy: 0.6000\n",
      "Epoch 161/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.6956 - accuracy: 0.6833 - val_loss: 0.7532 - val_accuracy: 0.6000\n",
      "Epoch 162/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6939 - accuracy: 0.6833 - val_loss: 0.7515 - val_accuracy: 0.6000\n",
      "Epoch 163/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.6922 - accuracy: 0.6833 - val_loss: 0.7496 - val_accuracy: 0.6000\n",
      "Epoch 164/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.6905 - accuracy: 0.6833 - val_loss: 0.7481 - val_accuracy: 0.6000\n",
      "Epoch 165/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.6888 - accuracy: 0.6833 - val_loss: 0.7464 - val_accuracy: 0.6000\n",
      "Epoch 166/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6871 - accuracy: 0.6833 - val_loss: 0.7448 - val_accuracy: 0.6000\n",
      "Epoch 167/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6855 - accuracy: 0.6833 - val_loss: 0.7430 - val_accuracy: 0.6000\n",
      "Epoch 168/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6838 - accuracy: 0.6833 - val_loss: 0.7413 - val_accuracy: 0.6000\n",
      "Epoch 169/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6823 - accuracy: 0.6833 - val_loss: 0.7395 - val_accuracy: 0.6000\n",
      "Epoch 170/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6806 - accuracy: 0.6833 - val_loss: 0.7379 - val_accuracy: 0.6000\n",
      "Epoch 171/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6790 - accuracy: 0.6833 - val_loss: 0.7362 - val_accuracy: 0.6000\n",
      "Epoch 172/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.6774 - accuracy: 0.6833 - val_loss: 0.7347 - val_accuracy: 0.6000\n",
      "Epoch 173/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6758 - accuracy: 0.6833 - val_loss: 0.7331 - val_accuracy: 0.6000\n",
      "Epoch 174/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6742 - accuracy: 0.6833 - val_loss: 0.7315 - val_accuracy: 0.6000\n",
      "Epoch 175/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.6727 - accuracy: 0.6833 - val_loss: 0.7299 - val_accuracy: 0.6000\n",
      "Epoch 176/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6711 - accuracy: 0.6833 - val_loss: 0.7283 - val_accuracy: 0.6000\n",
      "Epoch 177/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6696 - accuracy: 0.6833 - val_loss: 0.7268 - val_accuracy: 0.6000\n",
      "Epoch 178/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6680 - accuracy: 0.6833 - val_loss: 0.7250 - val_accuracy: 0.6000\n",
      "Epoch 179/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6665 - accuracy: 0.6833 - val_loss: 0.7235 - val_accuracy: 0.6000\n",
      "Epoch 180/300\n",
      "120/120 [==============================] - 0s 147us/sample - loss: 0.6650 - accuracy: 0.6833 - val_loss: 0.7220 - val_accuracy: 0.6000\n",
      "Epoch 181/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.6635 - accuracy: 0.6833 - val_loss: 0.7202 - val_accuracy: 0.6000\n",
      "Epoch 182/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6621 - accuracy: 0.6833 - val_loss: 0.7185 - val_accuracy: 0.6000\n",
      "Epoch 183/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6606 - accuracy: 0.6833 - val_loss: 0.7170 - val_accuracy: 0.6000\n",
      "Epoch 184/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.6591 - accuracy: 0.6833 - val_loss: 0.7155 - val_accuracy: 0.6000\n",
      "Epoch 185/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.6576 - accuracy: 0.6833 - val_loss: 0.7140 - val_accuracy: 0.6000\n",
      "Epoch 186/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.6562 - accuracy: 0.6833 - val_loss: 0.7123 - val_accuracy: 0.6000\n",
      "Epoch 187/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6547 - accuracy: 0.6833 - val_loss: 0.7109 - val_accuracy: 0.6000\n",
      "Epoch 188/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.6533 - accuracy: 0.6833 - val_loss: 0.7096 - val_accuracy: 0.6000\n",
      "Epoch 189/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6520 - accuracy: 0.6833 - val_loss: 0.7080 - val_accuracy: 0.6000\n",
      "Epoch 190/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6506 - accuracy: 0.6833 - val_loss: 0.7066 - val_accuracy: 0.6000\n",
      "Epoch 191/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.6491 - accuracy: 0.6833 - val_loss: 0.7054 - val_accuracy: 0.6000\n",
      "Epoch 192/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6477 - accuracy: 0.6833 - val_loss: 0.7042 - val_accuracy: 0.6000\n",
      "Epoch 193/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.6464 - accuracy: 0.6833 - val_loss: 0.7028 - val_accuracy: 0.6000\n",
      "Epoch 194/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6450 - accuracy: 0.6833 - val_loss: 0.7011 - val_accuracy: 0.6000\n",
      "Epoch 195/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6436 - accuracy: 0.6833 - val_loss: 0.6997 - val_accuracy: 0.6000\n",
      "Epoch 196/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6424 - accuracy: 0.6833 - val_loss: 0.6980 - val_accuracy: 0.6000\n",
      "Epoch 197/300\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.6409 - accuracy: 0.6833 - val_loss: 0.6966 - val_accuracy: 0.6000\n",
      "Epoch 198/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6396 - accuracy: 0.6833 - val_loss: 0.6953 - val_accuracy: 0.6000\n",
      "Epoch 199/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6383 - accuracy: 0.6833 - val_loss: 0.6938 - val_accuracy: 0.6000\n",
      "Epoch 200/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6370 - accuracy: 0.6833 - val_loss: 0.6926 - val_accuracy: 0.6000\n",
      "Epoch 201/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6357 - accuracy: 0.6833 - val_loss: 0.6912 - val_accuracy: 0.6000\n",
      "Epoch 202/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6344 - accuracy: 0.6833 - val_loss: 0.6900 - val_accuracy: 0.6000\n",
      "Epoch 203/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6331 - accuracy: 0.6833 - val_loss: 0.6885 - val_accuracy: 0.6000\n",
      "Epoch 204/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6318 - accuracy: 0.6833 - val_loss: 0.6871 - val_accuracy: 0.6000\n",
      "Epoch 205/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6306 - accuracy: 0.6833 - val_loss: 0.6858 - val_accuracy: 0.6000\n",
      "Epoch 206/300\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.6294 - accuracy: 0.6833 - val_loss: 0.6846 - val_accuracy: 0.6000\n",
      "Epoch 207/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.6281 - accuracy: 0.6833 - val_loss: 0.6829 - val_accuracy: 0.6000\n",
      "Epoch 208/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.6269 - accuracy: 0.6833 - val_loss: 0.6814 - val_accuracy: 0.6000\n",
      "Epoch 209/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6257 - accuracy: 0.6833 - val_loss: 0.6803 - val_accuracy: 0.6000\n",
      "Epoch 210/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6243 - accuracy: 0.6833 - val_loss: 0.6790 - val_accuracy: 0.6000\n",
      "Epoch 211/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6231 - accuracy: 0.6833 - val_loss: 0.6776 - val_accuracy: 0.6000\n",
      "Epoch 212/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.6220 - accuracy: 0.6833 - val_loss: 0.6762 - val_accuracy: 0.6000\n",
      "Epoch 213/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6208 - accuracy: 0.6833 - val_loss: 0.6749 - val_accuracy: 0.6000\n",
      "Epoch 214/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6196 - accuracy: 0.6833 - val_loss: 0.6738 - val_accuracy: 0.6000\n",
      "Epoch 215/300\n",
      "120/120 [==============================] - 0s 153us/sample - loss: 0.6185 - accuracy: 0.6833 - val_loss: 0.6724 - val_accuracy: 0.6000\n",
      "Epoch 216/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6172 - accuracy: 0.6833 - val_loss: 0.6714 - val_accuracy: 0.6000\n",
      "Epoch 217/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6161 - accuracy: 0.6833 - val_loss: 0.6701 - val_accuracy: 0.6000\n",
      "Epoch 218/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6149 - accuracy: 0.6833 - val_loss: 0.6690 - val_accuracy: 0.6000\n",
      "Epoch 219/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6138 - accuracy: 0.6833 - val_loss: 0.6679 - val_accuracy: 0.6000\n",
      "Epoch 220/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6127 - accuracy: 0.6833 - val_loss: 0.6667 - val_accuracy: 0.6000\n",
      "Epoch 221/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6115 - accuracy: 0.6833 - val_loss: 0.6658 - val_accuracy: 0.6000\n",
      "Epoch 222/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6104 - accuracy: 0.6833 - val_loss: 0.6647 - val_accuracy: 0.6000\n",
      "Epoch 223/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6093 - accuracy: 0.6833 - val_loss: 0.6635 - val_accuracy: 0.6000\n",
      "Epoch 224/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6082 - accuracy: 0.6833 - val_loss: 0.6622 - val_accuracy: 0.6000\n",
      "Epoch 225/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6071 - accuracy: 0.6833 - val_loss: 0.6611 - val_accuracy: 0.6000\n",
      "Epoch 226/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6060 - accuracy: 0.6833 - val_loss: 0.6600 - val_accuracy: 0.6000\n",
      "Epoch 227/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.6048 - accuracy: 0.6833 - val_loss: 0.6588 - val_accuracy: 0.6000\n",
      "Epoch 228/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.6038 - accuracy: 0.6833 - val_loss: 0.6576 - val_accuracy: 0.6000\n",
      "Epoch 229/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.6027 - accuracy: 0.6833 - val_loss: 0.6564 - val_accuracy: 0.6000\n",
      "Epoch 230/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.6016 - accuracy: 0.6833 - val_loss: 0.6552 - val_accuracy: 0.6000\n",
      "Epoch 231/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.6006 - accuracy: 0.6833 - val_loss: 0.6540 - val_accuracy: 0.6000\n",
      "Epoch 232/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5995 - accuracy: 0.6833 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
      "Epoch 233/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5985 - accuracy: 0.6833 - val_loss: 0.6519 - val_accuracy: 0.6000\n",
      "Epoch 234/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5974 - accuracy: 0.6833 - val_loss: 0.6508 - val_accuracy: 0.6000\n",
      "Epoch 235/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5964 - accuracy: 0.6833 - val_loss: 0.6497 - val_accuracy: 0.6000\n",
      "Epoch 236/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5953 - accuracy: 0.6833 - val_loss: 0.6487 - val_accuracy: 0.6000\n",
      "Epoch 237/300\n",
      "120/120 [==============================] - 0s 167us/sample - loss: 0.5943 - accuracy: 0.6833 - val_loss: 0.6478 - val_accuracy: 0.6000\n",
      "Epoch 238/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5933 - accuracy: 0.6833 - val_loss: 0.6466 - val_accuracy: 0.6000\n",
      "Epoch 239/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5923 - accuracy: 0.6833 - val_loss: 0.6456 - val_accuracy: 0.6000\n",
      "Epoch 240/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.5914 - accuracy: 0.6833 - val_loss: 0.6448 - val_accuracy: 0.6000\n",
      "Epoch 241/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5902 - accuracy: 0.6833 - val_loss: 0.6438 - val_accuracy: 0.6000\n",
      "Epoch 242/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5893 - accuracy: 0.6833 - val_loss: 0.6426 - val_accuracy: 0.6000\n",
      "Epoch 243/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5883 - accuracy: 0.6833 - val_loss: 0.6415 - val_accuracy: 0.6000\n",
      "Epoch 244/300\n",
      "120/120 [==============================] - 0s 137us/sample - loss: 0.5873 - accuracy: 0.6833 - val_loss: 0.6407 - val_accuracy: 0.6000\n",
      "Epoch 245/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5863 - accuracy: 0.6833 - val_loss: 0.6396 - val_accuracy: 0.6000\n",
      "Epoch 246/300\n",
      "120/120 [==============================] - 0s 149us/sample - loss: 0.5853 - accuracy: 0.6833 - val_loss: 0.6385 - val_accuracy: 0.6000\n",
      "Epoch 247/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.5843 - accuracy: 0.6833 - val_loss: 0.6375 - val_accuracy: 0.6000\n",
      "Epoch 248/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5834 - accuracy: 0.6833 - val_loss: 0.6364 - val_accuracy: 0.6000\n",
      "Epoch 249/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5824 - accuracy: 0.6917 - val_loss: 0.6352 - val_accuracy: 0.6000\n",
      "Epoch 250/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5815 - accuracy: 0.6917 - val_loss: 0.6342 - val_accuracy: 0.6000\n",
      "Epoch 251/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5805 - accuracy: 0.6917 - val_loss: 0.6331 - val_accuracy: 0.6000\n",
      "Epoch 252/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5795 - accuracy: 0.6917 - val_loss: 0.6320 - val_accuracy: 0.6000\n",
      "Epoch 253/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5786 - accuracy: 0.6917 - val_loss: 0.6311 - val_accuracy: 0.6000\n",
      "Epoch 254/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5777 - accuracy: 0.6917 - val_loss: 0.6300 - val_accuracy: 0.6000\n",
      "Epoch 255/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5768 - accuracy: 0.6917 - val_loss: 0.6290 - val_accuracy: 0.6000\n",
      "Epoch 256/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5758 - accuracy: 0.6917 - val_loss: 0.6282 - val_accuracy: 0.6000\n",
      "Epoch 257/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5750 - accuracy: 0.6917 - val_loss: 0.6271 - val_accuracy: 0.6000\n",
      "Epoch 258/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5740 - accuracy: 0.6917 - val_loss: 0.6264 - val_accuracy: 0.6000\n",
      "Epoch 259/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5731 - accuracy: 0.6917 - val_loss: 0.6253 - val_accuracy: 0.6000\n",
      "Epoch 260/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5722 - accuracy: 0.6917 - val_loss: 0.6243 - val_accuracy: 0.6000\n",
      "Epoch 261/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.5712 - accuracy: 0.6917 - val_loss: 0.6233 - val_accuracy: 0.6000\n",
      "Epoch 262/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5703 - accuracy: 0.6917 - val_loss: 0.6223 - val_accuracy: 0.6000\n",
      "Epoch 263/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5695 - accuracy: 0.6917 - val_loss: 0.6211 - val_accuracy: 0.6000\n",
      "Epoch 264/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5686 - accuracy: 0.6917 - val_loss: 0.6201 - val_accuracy: 0.6000\n",
      "Epoch 265/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5677 - accuracy: 0.6917 - val_loss: 0.6191 - val_accuracy: 0.6000\n",
      "Epoch 266/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5668 - accuracy: 0.6917 - val_loss: 0.6182 - val_accuracy: 0.6333\n",
      "Epoch 267/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5660 - accuracy: 0.6917 - val_loss: 0.6170 - val_accuracy: 0.6333\n",
      "Epoch 268/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5651 - accuracy: 0.6917 - val_loss: 0.6162 - val_accuracy: 0.6333\n",
      "Epoch 269/300\n",
      "120/120 [==============================] - 0s 157us/sample - loss: 0.5642 - accuracy: 0.6917 - val_loss: 0.6155 - val_accuracy: 0.6333\n",
      "Epoch 270/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5633 - accuracy: 0.6917 - val_loss: 0.6145 - val_accuracy: 0.6333\n",
      "Epoch 271/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5625 - accuracy: 0.6917 - val_loss: 0.6136 - val_accuracy: 0.6333\n",
      "Epoch 272/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5616 - accuracy: 0.6917 - val_loss: 0.6128 - val_accuracy: 0.6333\n",
      "Epoch 273/300\n",
      "120/120 [==============================] - 0s 138us/sample - loss: 0.5608 - accuracy: 0.6917 - val_loss: 0.6118 - val_accuracy: 0.6333\n",
      "Epoch 274/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5599 - accuracy: 0.6917 - val_loss: 0.6110 - val_accuracy: 0.6333\n",
      "Epoch 275/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.5591 - accuracy: 0.7000 - val_loss: 0.6099 - val_accuracy: 0.6333\n",
      "Epoch 276/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5582 - accuracy: 0.7000 - val_loss: 0.6089 - val_accuracy: 0.6333\n",
      "Epoch 277/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5574 - accuracy: 0.7000 - val_loss: 0.6076 - val_accuracy: 0.6333\n",
      "Epoch 278/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5566 - accuracy: 0.7000 - val_loss: 0.6067 - val_accuracy: 0.6333\n",
      "Epoch 279/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5557 - accuracy: 0.7000 - val_loss: 0.6057 - val_accuracy: 0.6333\n",
      "Epoch 280/300\n",
      "120/120 [==============================] - 0s 143us/sample - loss: 0.5549 - accuracy: 0.7000 - val_loss: 0.6048 - val_accuracy: 0.6333\n",
      "Epoch 281/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5541 - accuracy: 0.7000 - val_loss: 0.6035 - val_accuracy: 0.6333\n",
      "Epoch 282/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5532 - accuracy: 0.7000 - val_loss: 0.6023 - val_accuracy: 0.6333\n",
      "Epoch 283/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5524 - accuracy: 0.7000 - val_loss: 0.6013 - val_accuracy: 0.6333\n",
      "Epoch 284/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5516 - accuracy: 0.7000 - val_loss: 0.6004 - val_accuracy: 0.6333\n",
      "Epoch 285/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5508 - accuracy: 0.7000 - val_loss: 0.5995 - val_accuracy: 0.6333\n",
      "Epoch 286/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5500 - accuracy: 0.7000 - val_loss: 0.5990 - val_accuracy: 0.6333\n",
      "Epoch 287/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5492 - accuracy: 0.7000 - val_loss: 0.5980 - val_accuracy: 0.6333\n",
      "Epoch 288/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5484 - accuracy: 0.7000 - val_loss: 0.5975 - val_accuracy: 0.6333\n",
      "Epoch 289/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5476 - accuracy: 0.7000 - val_loss: 0.5965 - val_accuracy: 0.6333\n",
      "Epoch 290/300\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.5468 - accuracy: 0.7000 - val_loss: 0.5958 - val_accuracy: 0.6333\n",
      "Epoch 291/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5460 - accuracy: 0.7000 - val_loss: 0.5949 - val_accuracy: 0.6333\n",
      "Epoch 292/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5453 - accuracy: 0.7000 - val_loss: 0.5940 - val_accuracy: 0.6333\n",
      "Epoch 293/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5445 - accuracy: 0.7000 - val_loss: 0.5928 - val_accuracy: 0.6333\n",
      "Epoch 294/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5437 - accuracy: 0.7000 - val_loss: 0.5919 - val_accuracy: 0.6333\n",
      "Epoch 295/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5429 - accuracy: 0.7000 - val_loss: 0.5910 - val_accuracy: 0.6333\n",
      "Epoch 296/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5421 - accuracy: 0.7083 - val_loss: 0.5903 - val_accuracy: 0.6333\n",
      "Epoch 297/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5414 - accuracy: 0.7000 - val_loss: 0.5898 - val_accuracy: 0.6333\n",
      "Epoch 298/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5406 - accuracy: 0.7083 - val_loss: 0.5890 - val_accuracy: 0.6333\n",
      "Epoch 299/300\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.5400 - accuracy: 0.7083 - val_loss: 0.5879 - val_accuracy: 0.6333\n",
      "Epoch 300/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5391 - accuracy: 0.7083 - val_loss: 0.5872 - val_accuracy: 0.6333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28ff26e9d48>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=scaled_X_train, \n",
    "          y=y_train, \n",
    "          epochs=300,\n",
    "          validation_data=(scaled_X_test, y_test), verbose=1 ,callbacks=[early_stop]         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.124315</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>1.101527</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.118055</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>1.098044</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.112693</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.094468</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.107052</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.090874</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.101796</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.087489</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.542123</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.590314</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.541408</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.589807</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.540605</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.589002</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.539951</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.587918</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.539108</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.587196</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy  val_loss  val_accuracy\n",
       "0    1.124315  0.325000  1.101527      0.400000\n",
       "1    1.118055  0.316667  1.098044      0.366667\n",
       "2    1.112693  0.300000  1.094468      0.366667\n",
       "3    1.107052  0.300000  1.090874      0.400000\n",
       "4    1.101796  0.300000  1.087489      0.400000\n",
       "..        ...       ...       ...           ...\n",
       "295  0.542123  0.708333  0.590314      0.633333\n",
       "296  0.541408  0.700000  0.589807      0.633333\n",
       "297  0.540605  0.708333  0.589002      0.633333\n",
       "298  0.539951  0.708333  0.587918      0.633333\n",
       "299  0.539108  0.708333  0.587196      0.633333\n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x28ff29d3448>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x290986af8c8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics[['accuracy','val_accuracy']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5871963500976562, 0.6333333]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(scaled_X_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = len(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=4,activation='relu'))\n",
    "\n",
    "# Last layer for multi-class classification of 3 species\n",
    "model.add(Dense(units=3,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples\n",
      "Epoch 1/300\n",
      "150/150 [==============================] - 0s 2ms/sample - loss: 1.3862 - accuracy: 0.3333\n",
      "Epoch 2/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 1.3601 - accuracy: 0.3333\n",
      "Epoch 3/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 1.3397 - accuracy: 0.3333\n",
      "Epoch 4/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 1.3139 - accuracy: 0.3400\n",
      "Epoch 5/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 1.2910 - accuracy: 0.3467\n",
      "Epoch 6/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 1.2682 - accuracy: 0.3467\n",
      "Epoch 7/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 1.2500 - accuracy: 0.3533\n",
      "Epoch 8/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 1.2290 - accuracy: 0.3867\n",
      "Epoch 9/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 1.2097 - accuracy: 0.4333\n",
      "Epoch 10/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 1.1926 - accuracy: 0.4533\n",
      "Epoch 11/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 1.1748 - accuracy: 0.4933\n",
      "Epoch 12/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 1.1582 - accuracy: 0.5333\n",
      "Epoch 13/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 1.1430 - accuracy: 0.5733\n",
      "Epoch 14/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 1.1289 - accuracy: 0.5933\n",
      "Epoch 15/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 1.1152 - accuracy: 0.5933\n",
      "Epoch 16/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 1.1030 - accuracy: 0.6000\n",
      "Epoch 17/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 1.0906 - accuracy: 0.5933\n",
      "Epoch 18/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 1.0806 - accuracy: 0.6067\n",
      "Epoch 19/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 1.0702 - accuracy: 0.5933\n",
      "Epoch 20/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 1.0604 - accuracy: 0.5867\n",
      "Epoch 21/300\n",
      "150/150 [==============================] - 0s 127us/sample - loss: 1.0517 - accuracy: 0.6133\n",
      "Epoch 22/300\n",
      "150/150 [==============================] - 0s 127us/sample - loss: 1.0437 - accuracy: 0.6133\n",
      "Epoch 23/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 1.0360 - accuracy: 0.6133\n",
      "Epoch 24/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 1.0288 - accuracy: 0.6200\n",
      "Epoch 25/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 1.0219 - accuracy: 0.6133\n",
      "Epoch 26/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 1.0153 - accuracy: 0.6333\n",
      "Epoch 27/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 1.0104 - accuracy: 0.6333\n",
      "Epoch 28/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 1.0039 - accuracy: 0.6400\n",
      "Epoch 29/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.9988 - accuracy: 0.6733\n",
      "Epoch 30/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.9938 - accuracy: 0.6667\n",
      "Epoch 31/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.9887 - accuracy: 0.6667\n",
      "Epoch 32/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.9839 - accuracy: 0.6733\n",
      "Epoch 33/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.9793 - accuracy: 0.6667\n",
      "Epoch 34/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.9750 - accuracy: 0.6600\n",
      "Epoch 35/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.9707 - accuracy: 0.6600\n",
      "Epoch 36/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.9662 - accuracy: 0.6600\n",
      "Epoch 37/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.9617 - accuracy: 0.6533\n",
      "Epoch 38/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.9578 - accuracy: 0.6533\n",
      "Epoch 39/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.9535 - accuracy: 0.6533\n",
      "Epoch 40/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.9494 - accuracy: 0.6533\n",
      "Epoch 41/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.9456 - accuracy: 0.6533\n",
      "Epoch 42/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.9417 - accuracy: 0.6467\n",
      "Epoch 43/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.9381 - accuracy: 0.6467\n",
      "Epoch 44/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.9346 - accuracy: 0.6467\n",
      "Epoch 45/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.9311 - accuracy: 0.6467\n",
      "Epoch 46/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.9276 - accuracy: 0.6467\n",
      "Epoch 47/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.9243 - accuracy: 0.6467\n",
      "Epoch 48/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.9208 - accuracy: 0.6467\n",
      "Epoch 49/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.9174 - accuracy: 0.6467\n",
      "Epoch 50/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.9140 - accuracy: 0.6467\n",
      "Epoch 51/300\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.9106 - accuracy: 0.6467\n",
      "Epoch 52/300\n",
      "150/150 [==============================] - 0s 133us/sample - loss: 0.9073 - accuracy: 0.6467\n",
      "Epoch 53/300\n",
      "150/150 [==============================] - 0s 127us/sample - loss: 0.9040 - accuracy: 0.6467\n",
      "Epoch 54/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.9005 - accuracy: 0.6467\n",
      "Epoch 55/300\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.8973 - accuracy: 0.6467\n",
      "Epoch 56/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.8939 - accuracy: 0.6467\n",
      "Epoch 57/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8905 - accuracy: 0.6467\n",
      "Epoch 58/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.8872 - accuracy: 0.6467\n",
      "Epoch 59/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8837 - accuracy: 0.6467\n",
      "Epoch 60/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.8803 - accuracy: 0.6467\n",
      "Epoch 61/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.8769 - accuracy: 0.6467\n",
      "Epoch 62/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.8735 - accuracy: 0.6467\n",
      "Epoch 63/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8701 - accuracy: 0.6467\n",
      "Epoch 64/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8667 - accuracy: 0.6467\n",
      "Epoch 65/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.8632 - accuracy: 0.6467\n",
      "Epoch 66/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8600 - accuracy: 0.6467\n",
      "Epoch 67/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8565 - accuracy: 0.6533\n",
      "Epoch 68/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.8531 - accuracy: 0.6533\n",
      "Epoch 69/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8497 - accuracy: 0.6533\n",
      "Epoch 70/300\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.8462 - accuracy: 0.6533\n",
      "Epoch 71/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.8427 - accuracy: 0.6533\n",
      "Epoch 72/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.8394 - accuracy: 0.6533\n",
      "Epoch 73/300\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.8359 - accuracy: 0.6533\n",
      "Epoch 74/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8324 - accuracy: 0.6533\n",
      "Epoch 75/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.8289 - accuracy: 0.6600\n",
      "Epoch 76/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.8255 - accuracy: 0.6600\n",
      "Epoch 77/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8221 - accuracy: 0.6600\n",
      "Epoch 78/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8186 - accuracy: 0.6600\n",
      "Epoch 79/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.8152 - accuracy: 0.6600\n",
      "Epoch 80/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8116 - accuracy: 0.6600\n",
      "Epoch 81/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.8083 - accuracy: 0.6600\n",
      "Epoch 82/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8047 - accuracy: 0.6600\n",
      "Epoch 83/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.8013 - accuracy: 0.6600\n",
      "Epoch 84/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.7977 - accuracy: 0.6600\n",
      "Epoch 85/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.7943 - accuracy: 0.6600\n",
      "Epoch 86/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7909 - accuracy: 0.6600\n",
      "Epoch 87/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.7874 - accuracy: 0.6600\n",
      "Epoch 88/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.7841 - accuracy: 0.6600\n",
      "Epoch 89/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.7804 - accuracy: 0.6600\n",
      "Epoch 90/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.7771 - accuracy: 0.6600\n",
      "Epoch 91/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7736 - accuracy: 0.6600\n",
      "Epoch 92/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.7701 - accuracy: 0.6600\n",
      "Epoch 93/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.7667 - accuracy: 0.6600\n",
      "Epoch 94/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7633 - accuracy: 0.6600\n",
      "Epoch 95/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.7597 - accuracy: 0.6600\n",
      "Epoch 96/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7563 - accuracy: 0.6600\n",
      "Epoch 97/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7529 - accuracy: 0.6600\n",
      "Epoch 98/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7494 - accuracy: 0.6600\n",
      "Epoch 99/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.7460 - accuracy: 0.6600\n",
      "Epoch 100/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.7427 - accuracy: 0.6600\n",
      "Epoch 101/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7393 - accuracy: 0.6600\n",
      "Epoch 102/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7358 - accuracy: 0.6600\n",
      "Epoch 103/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.7325 - accuracy: 0.6667\n",
      "Epoch 104/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7292 - accuracy: 0.6667\n",
      "Epoch 105/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7257 - accuracy: 0.6667\n",
      "Epoch 106/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7224 - accuracy: 0.6667\n",
      "Epoch 107/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.7192 - accuracy: 0.6667\n",
      "Epoch 108/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.7158 - accuracy: 0.6667\n",
      "Epoch 109/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.7125 - accuracy: 0.6667\n",
      "Epoch 110/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7092 - accuracy: 0.6667\n",
      "Epoch 111/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7060 - accuracy: 0.6667\n",
      "Epoch 112/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7029 - accuracy: 0.6667\n",
      "Epoch 113/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6995 - accuracy: 0.6667\n",
      "Epoch 114/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6964 - accuracy: 0.6667\n",
      "Epoch 115/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6932 - accuracy: 0.6667\n",
      "Epoch 116/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6899 - accuracy: 0.6667\n",
      "Epoch 117/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6867 - accuracy: 0.6733\n",
      "Epoch 118/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6835 - accuracy: 0.6733\n",
      "Epoch 119/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6804 - accuracy: 0.6733\n",
      "Epoch 120/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6773 - accuracy: 0.6800\n",
      "Epoch 121/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6742 - accuracy: 0.6800\n",
      "Epoch 122/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6711 - accuracy: 0.6800\n",
      "Epoch 123/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6681 - accuracy: 0.6800\n",
      "Epoch 124/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6653 - accuracy: 0.6800\n",
      "Epoch 125/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6619 - accuracy: 0.6800\n",
      "Epoch 126/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6591 - accuracy: 0.6800\n",
      "Epoch 127/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6561 - accuracy: 0.6800\n",
      "Epoch 128/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6531 - accuracy: 0.6800\n",
      "Epoch 129/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6504 - accuracy: 0.6800\n",
      "Epoch 130/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6472 - accuracy: 0.6800\n",
      "Epoch 131/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6443 - accuracy: 0.6800\n",
      "Epoch 132/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6416 - accuracy: 0.6800\n",
      "Epoch 133/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6386 - accuracy: 0.6800\n",
      "Epoch 134/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6359 - accuracy: 0.6800\n",
      "Epoch 135/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6330 - accuracy: 0.6867\n",
      "Epoch 136/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6301 - accuracy: 0.6867\n",
      "Epoch 137/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6274 - accuracy: 0.6800\n",
      "Epoch 138/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6247 - accuracy: 0.6800\n",
      "Epoch 139/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6219 - accuracy: 0.6800\n",
      "Epoch 140/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6193 - accuracy: 0.6800\n",
      "Epoch 141/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6166 - accuracy: 0.6800\n",
      "Epoch 142/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6139 - accuracy: 0.6867\n",
      "Epoch 143/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6113 - accuracy: 0.6867\n",
      "Epoch 144/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6087 - accuracy: 0.6867\n",
      "Epoch 145/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6061 - accuracy: 0.6933\n",
      "Epoch 146/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6036 - accuracy: 0.7000\n",
      "Epoch 147/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6010 - accuracy: 0.7000\n",
      "Epoch 148/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5984 - accuracy: 0.7000\n",
      "Epoch 149/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5959 - accuracy: 0.7067\n",
      "Epoch 150/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5934 - accuracy: 0.7133\n",
      "Epoch 151/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5910 - accuracy: 0.7133\n",
      "Epoch 152/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5885 - accuracy: 0.7133\n",
      "Epoch 153/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5862 - accuracy: 0.7133\n",
      "Epoch 154/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5838 - accuracy: 0.7133\n",
      "Epoch 155/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5815 - accuracy: 0.7133\n",
      "Epoch 156/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5790 - accuracy: 0.7133\n",
      "Epoch 157/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5767 - accuracy: 0.7133\n",
      "Epoch 158/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5744 - accuracy: 0.7133\n",
      "Epoch 159/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5721 - accuracy: 0.7200\n",
      "Epoch 160/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5698 - accuracy: 0.7200\n",
      "Epoch 161/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5675 - accuracy: 0.7200\n",
      "Epoch 162/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5653 - accuracy: 0.7200\n",
      "Epoch 163/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5631 - accuracy: 0.7200\n",
      "Epoch 164/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5609 - accuracy: 0.7333\n",
      "Epoch 165/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5588 - accuracy: 0.7333\n",
      "Epoch 166/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5566 - accuracy: 0.7267\n",
      "Epoch 167/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5544 - accuracy: 0.7400\n",
      "Epoch 168/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5523 - accuracy: 0.7467\n",
      "Epoch 169/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5502 - accuracy: 0.7467\n",
      "Epoch 170/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5481 - accuracy: 0.7467\n",
      "Epoch 171/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5462 - accuracy: 0.7467\n",
      "Epoch 172/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5440 - accuracy: 0.7467\n",
      "Epoch 173/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5420 - accuracy: 0.7467\n",
      "Epoch 174/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5399 - accuracy: 0.7467\n",
      "Epoch 175/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5380 - accuracy: 0.7533\n",
      "Epoch 176/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5360 - accuracy: 0.7533\n",
      "Epoch 177/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5340 - accuracy: 0.7533\n",
      "Epoch 178/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5321 - accuracy: 0.7600\n",
      "Epoch 179/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5302 - accuracy: 0.7667\n",
      "Epoch 180/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5283 - accuracy: 0.7667\n",
      "Epoch 181/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5264 - accuracy: 0.7733\n",
      "Epoch 182/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5245 - accuracy: 0.7800\n",
      "Epoch 183/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5227 - accuracy: 0.7800\n",
      "Epoch 184/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5208 - accuracy: 0.7933\n",
      "Epoch 185/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5190 - accuracy: 0.7933\n",
      "Epoch 186/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5172 - accuracy: 0.7933\n",
      "Epoch 187/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5154 - accuracy: 0.7933\n",
      "Epoch 188/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5136 - accuracy: 0.7933\n",
      "Epoch 189/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5118 - accuracy: 0.7933\n",
      "Epoch 190/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5100 - accuracy: 0.8067\n",
      "Epoch 191/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5083 - accuracy: 0.8200\n",
      "Epoch 192/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5066 - accuracy: 0.8267\n",
      "Epoch 193/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5050 - accuracy: 0.8467\n",
      "Epoch 194/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5032 - accuracy: 0.8467\n",
      "Epoch 195/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.5015 - accuracy: 0.8467\n",
      "Epoch 196/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4998 - accuracy: 0.8467\n",
      "Epoch 197/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4982 - accuracy: 0.8667\n",
      "Epoch 198/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4966 - accuracy: 0.8667\n",
      "Epoch 199/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4949 - accuracy: 0.8667\n",
      "Epoch 200/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4933 - accuracy: 0.8667\n",
      "Epoch 201/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4917 - accuracy: 0.8733\n",
      "Epoch 202/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4901 - accuracy: 0.8733\n",
      "Epoch 203/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.4885 - accuracy: 0.8733\n",
      "Epoch 204/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.4869 - accuracy: 0.8800\n",
      "Epoch 205/300\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.4854 - accuracy: 0.8800\n",
      "Epoch 206/300\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.4839 - accuracy: 0.8800\n",
      "Epoch 207/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4823 - accuracy: 0.8867\n",
      "Epoch 208/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4808 - accuracy: 0.8867\n",
      "Epoch 209/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4793 - accuracy: 0.8800\n",
      "Epoch 210/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4779 - accuracy: 0.8933\n",
      "Epoch 211/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.4764 - accuracy: 0.8933\n",
      "Epoch 212/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4748 - accuracy: 0.8933\n",
      "Epoch 213/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4733 - accuracy: 0.8933\n",
      "Epoch 214/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4719 - accuracy: 0.8933\n",
      "Epoch 215/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4708 - accuracy: 0.8933\n",
      "Epoch 216/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4690 - accuracy: 0.8933\n",
      "Epoch 217/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4676 - accuracy: 0.8933\n",
      "Epoch 218/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4662 - accuracy: 0.8933\n",
      "Epoch 219/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4648 - accuracy: 0.8933\n",
      "Epoch 220/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4633 - accuracy: 0.9000\n",
      "Epoch 221/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4620 - accuracy: 0.9067\n",
      "Epoch 222/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4608 - accuracy: 0.9067\n",
      "Epoch 223/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.4593 - accuracy: 0.9067\n",
      "Epoch 224/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4579 - accuracy: 0.9067\n",
      "Epoch 225/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4566 - accuracy: 0.9067\n",
      "Epoch 226/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4552 - accuracy: 0.9133\n",
      "Epoch 227/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4539 - accuracy: 0.9133\n",
      "Epoch 228/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.4526 - accuracy: 0.9133\n",
      "Epoch 229/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4513 - accuracy: 0.9133\n",
      "Epoch 230/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4499 - accuracy: 0.9133\n",
      "Epoch 231/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4486 - accuracy: 0.9133\n",
      "Epoch 232/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4473 - accuracy: 0.9133\n",
      "Epoch 233/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4460 - accuracy: 0.9133\n",
      "Epoch 234/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4447 - accuracy: 0.9133\n",
      "Epoch 235/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4435 - accuracy: 0.9133\n",
      "Epoch 236/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4422 - accuracy: 0.9133\n",
      "Epoch 237/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4409 - accuracy: 0.9133\n",
      "Epoch 238/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4396 - accuracy: 0.9267\n",
      "Epoch 239/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4387 - accuracy: 0.9267\n",
      "Epoch 240/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4375 - accuracy: 0.9267\n",
      "Epoch 241/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4360 - accuracy: 0.9267\n",
      "Epoch 242/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.4347 - accuracy: 0.9333\n",
      "Epoch 243/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4335 - accuracy: 0.9333\n",
      "Epoch 244/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4323 - accuracy: 0.9400\n",
      "Epoch 245/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4311 - accuracy: 0.9400\n",
      "Epoch 246/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4299 - accuracy: 0.9400\n",
      "Epoch 247/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4287 - accuracy: 0.9333\n",
      "Epoch 248/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4275 - accuracy: 0.9333\n",
      "Epoch 249/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4264 - accuracy: 0.9333\n",
      "Epoch 250/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4252 - accuracy: 0.9333\n",
      "Epoch 251/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.4240 - accuracy: 0.9400\n",
      "Epoch 252/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4228 - accuracy: 0.9400\n",
      "Epoch 253/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4217 - accuracy: 0.9400\n",
      "Epoch 254/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4206 - accuracy: 0.9400\n",
      "Epoch 255/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4194 - accuracy: 0.9400\n",
      "Epoch 256/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.4183 - accuracy: 0.9400\n",
      "Epoch 257/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4172 - accuracy: 0.9400\n",
      "Epoch 258/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4160 - accuracy: 0.9400\n",
      "Epoch 259/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4148 - accuracy: 0.9400\n",
      "Epoch 260/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.4139 - accuracy: 0.9400\n",
      "Epoch 261/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4126 - accuracy: 0.9400\n",
      "Epoch 262/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4115 - accuracy: 0.9400\n",
      "Epoch 263/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4104 - accuracy: 0.9400\n",
      "Epoch 264/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4094 - accuracy: 0.9400\n",
      "Epoch 265/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4082 - accuracy: 0.9400\n",
      "Epoch 266/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4072 - accuracy: 0.9400\n",
      "Epoch 267/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4061 - accuracy: 0.9400\n",
      "Epoch 268/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4050 - accuracy: 0.9400\n",
      "Epoch 269/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4039 - accuracy: 0.9400\n",
      "Epoch 270/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4029 - accuracy: 0.9400\n",
      "Epoch 271/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4018 - accuracy: 0.9400\n",
      "Epoch 272/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4008 - accuracy: 0.9400\n",
      "Epoch 273/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3997 - accuracy: 0.9400\n",
      "Epoch 274/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3988 - accuracy: 0.9400\n",
      "Epoch 275/300\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.3976 - accuracy: 0.9400\n",
      "Epoch 276/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3966 - accuracy: 0.9400\n",
      "Epoch 277/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3955 - accuracy: 0.9400\n",
      "Epoch 278/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3945 - accuracy: 0.9400\n",
      "Epoch 279/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3935 - accuracy: 0.9400\n",
      "Epoch 280/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3924 - accuracy: 0.9400\n",
      "Epoch 281/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3914 - accuracy: 0.9400\n",
      "Epoch 282/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3904 - accuracy: 0.9400\n",
      "Epoch 283/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3894 - accuracy: 0.9400\n",
      "Epoch 284/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3884 - accuracy: 0.9400\n",
      "Epoch 285/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3873 - accuracy: 0.9400\n",
      "Epoch 286/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3863 - accuracy: 0.9467\n",
      "Epoch 287/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3854 - accuracy: 0.9467\n",
      "Epoch 288/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3843 - accuracy: 0.9467\n",
      "Epoch 289/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3836 - accuracy: 0.9467\n",
      "Epoch 290/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3824 - accuracy: 0.9400\n",
      "Epoch 291/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3814 - accuracy: 0.9467\n",
      "Epoch 292/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3804 - accuracy: 0.9467\n",
      "Epoch 293/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3795 - accuracy: 0.9467\n",
      "Epoch 294/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3784 - accuracy: 0.9467\n",
      "Epoch 295/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3774 - accuracy: 0.9467\n",
      "Epoch 296/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3765 - accuracy: 0.9467\n",
      "Epoch 297/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3758 - accuracy: 0.9467\n",
      "Epoch 298/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3747 - accuracy: 0.9467\n",
      "Epoch 299/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3737 - accuracy: 0.9467\n",
      "Epoch 300/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3726 - accuracy: 0.9467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29099ef7ac8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_X,y,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"final_iris_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_scaler.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler,'iris_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a Single New Flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "flower_model = load_model(\"final_iris_model.h5\")\n",
    "flower_scaler = joblib.load(\"iris_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_example = {'sepal_length':5.1,\n",
    "                 'sepal_width':3.5,\n",
    "                 'petal_length':1.4,\n",
    "                 'petal_width':0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flower_example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prediction(model,scaler,sample_json):\n",
    "    \n",
    "    # For larger data features, you should probably write a for loop\n",
    "    # That builds out this array for you\n",
    "    \n",
    "    s_len = sample_json['sepal_length']\n",
    "    s_wid = sample_json['sepal_width']\n",
    "    p_len = sample_json['petal_length']\n",
    "    p_wid = sample_json['petal_width']\n",
    "    \n",
    "    flower = [[s_len,s_wid,p_len,p_wid]]\n",
    "    \n",
    "    flower = scaler.transform(flower)\n",
    "    \n",
    "    classes = np.array(['setosa', 'versicolor', 'virginica'])\n",
    "    \n",
    "    class_ind = model.predict_classes(flower)\n",
    "    \n",
    "    return classes[class_ind][0]\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_prediction(flower_model,flower_scaler,flower_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE FOR DEPLOYMENT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "\n",
    "flower_model = load_model(\"final_iris_model.h5\")\n",
    "flower_scaler = joblib.load(\"iris_scaler.pkl\")\n",
    "\n",
    "\n",
    "def return_prediction(model,scaler,sample_json):\n",
    "    \n",
    "    # For larger data features, you should probably write a for loop\n",
    "    # That builds out this array for you\n",
    "    \n",
    "    s_len = sample_json['sepal_length']\n",
    "    s_wid = sample_json['sepal_width']\n",
    "    p_len = sample_json['petal_length']\n",
    "    p_wid = sample_json['petal_width']\n",
    "    \n",
    "    flower = [[s_len,s_wid,p_len,p_wid]]\n",
    "    \n",
    "    flower = scaler.transform(flower)\n",
    "    \n",
    "    classes = np.array(['setosa', 'versicolor', 'virginica'])\n",
    "    \n",
    "    class_ind = model.predict_classes(flower)\n",
    "    \n",
    "    return classes[class_ind][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_example = {\n",
    "\"sepal_length\":5.1,\n",
    "\"sepal_width\":3.5,\n",
    "\"petal_length\":1.4,\n",
    "\"petal_width\":0.2\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
