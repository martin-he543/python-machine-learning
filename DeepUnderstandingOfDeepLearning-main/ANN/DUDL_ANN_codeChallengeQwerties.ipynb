{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "DUDL_ANN_codeChallengeQwerties.ipynb",
   "provenance": [
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1619331131782
    }
   ],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyNyqevL6vPNW79RM1uTontq"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: ANNs\n",
    "### LECTURE: CodeChallenge: more qwerties!\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBzskQUd2RK0"
   },
   "source": [
    "# Import and process the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MU7rvmWuhjud"
   },
   "source": [
    "# create data\n",
    "\n",
    "nPerClust = 100\n",
    "blur = 1\n",
    "\n",
    "A = [  1,  1 ]\n",
    "B = [  5,  1 ]\n",
    "C = [  3, -2 ]\n",
    "\n",
    "# generate data\n",
    "a = [ A[0]+np.random.randn(nPerClust)*blur , A[1]+np.random.randn(nPerClust)*blur ]\n",
    "b = [ B[0]+np.random.randn(nPerClust)*blur , B[1]+np.random.randn(nPerClust)*blur ]\n",
    "c = [ C[0]+np.random.randn(nPerClust)*blur , C[1]+np.random.randn(nPerClust)*blur ]\n",
    "\n",
    "# true labels\n",
    "labels_np = np.vstack((np.zeros((nPerClust,1)),np.ones((nPerClust,1)),1+np.ones((nPerClust,1))))\n",
    "\n",
    "# concatanate into a matrix\n",
    "data_np = np.hstack((a,b,c)).T\n",
    "\n",
    "# convert to a pytorch tensor\n",
    "data = torch.tensor(data_np).float()\n",
    "labels = torch.squeeze(torch.tensor(labels_np).long())\n",
    "\n",
    "# show the data\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.plot(data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'bs')\n",
    "plt.plot(data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'ko')\n",
    "plt.plot(data[np.where(labels==2)[0],0],data[np.where(labels==2)[0],1],'r^')\n",
    "plt.title('The qwerties!')\n",
    "plt.xlabel('qwerty dimension 1')\n",
    "plt.ylabel('qwerty dimension 2')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwusZlcu2VVS"
   },
   "source": [
    "# Create the ANN model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v0JMIGb1iV_9"
   },
   "source": [
    "# model architecture\n",
    "ANNq = nn.Sequential(\n",
    "    nn.Linear(2,4),    # input layer\n",
    "    nn.ReLU(),         # activation unit\n",
    "    nn.Linear(4,3),    # output units\n",
    "    nn.Softmax(dim=1), # final activation unit (see Additional Exploration #2 for a discussion about this)\n",
    "      )\n",
    "\n",
    "# loss function\n",
    "lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(ANNq.parameters(),lr=.01)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YcvZpU6_rUGO"
   },
   "source": [
    "# test the model\n",
    "yHat = ANNq(data)\n",
    "\n",
    "print(data.shape)\n",
    "print(yHat.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPJl5Fzw2Xgy"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cVD1nFTli7TO"
   },
   "source": [
    "numepochs = 10000\n",
    "\n",
    "# initialize losses\n",
    "losses = torch.zeros(numepochs)\n",
    "ongoingAcc = []\n",
    "\n",
    "# loop over epochs\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "  # forward pass\n",
    "  yHat = ANNq(data)\n",
    "\n",
    "  # compute loss\n",
    "  loss = lossfun(yHat,labels)\n",
    "  losses[epochi] = loss\n",
    "\n",
    "  # backprop\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  # compute accuracy\n",
    "  matches = torch.argmax(yHat,axis=1) == labels # booleans (false/true)\n",
    "  matchesNumeric = matches.float()              # convert to numbers (0/1)\n",
    "  accuracyPct = 100*torch.mean(matchesNumeric)  # average and x100 \n",
    "  ongoingAcc.append( accuracyPct )              # add to list of accuracies\n",
    "\n",
    "\n",
    "\n",
    "# final forward pass\n",
    "predictions = ANNq(data)\n",
    "  \n",
    "predlabels = torch.argmax(predictions,axis=1)\n",
    "totalacc = 100*torch.mean((predlabels == labels).float())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xa3RWuZ2adq"
   },
   "source": [
    "# Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JYouZAY4i3jM"
   },
   "source": [
    "# report accuracy\n",
    "print('Final accuracy: %g%%' %totalacc)\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(13,4))\n",
    "\n",
    "ax[0].plot(losses.detach())\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_title('Losses')\n",
    "\n",
    "ax[1].plot(ongoingAcc)\n",
    "ax[1].set_ylabel('accuracy')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_title('Accuracy')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "St6NI4qBk4tO"
   },
   "source": [
    "# confirm that all model predictions sum to 1\n",
    "torch.sum(yHat,axis=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gB8-aMAF_xIQ"
   },
   "source": [
    "# plot the raw model outputs\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "colorshape = [  'bs','ko','r^' ]\n",
    "for i in range(3):\n",
    "  plt.plot(yHat[:,i].detach(),colorshape[i],markerfacecolor='w')\n",
    "\n",
    "plt.xlabel('Stimulus number')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend(['qwert 1','qwert 2','qwert 2'],loc=(1.01,.4))\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zvLH4h6ek5Ax"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7MKj66sk5DY"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zIXi3fONIKVA"
   },
   "source": [
    "# 1) Does the model always do well? Re-run the entire notebook multiple times and see if it always reaches high accuracy\n",
    "#    (e.g., >90%). What do you think would be ways to improve the performance stability of the model?\n",
    "# \n",
    "# 2) You'll learn in the section \"Metaparameters\" that CrossEntropyLoss computes log-softmax internally. Does that mean \n",
    "#    that the Softmax() layer in the model needs to be there? Does it hurt or help? If you remove that final layer, what\n",
    "#    would change and what would be the same in the rest of the notebook?\n",
    "#    (Note about this problem: If it feels too advanced, then revisit this problem after the \"Metaparameters\" section.)\n",
    "# "
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}